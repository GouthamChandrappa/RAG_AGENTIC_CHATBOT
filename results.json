{
  "query": "what do i need to do according to the pdf?",
  "retrieval": {
    "documents": [
      {
        "text": " \nNew Hire Challenge  \n \nQuality Assurance Engineer \u2013 AI Product Line  \nAgentic RAG Chatbot Challenge with Crew AI, Chonkie, and DeepEval  \n \nObjective:  \nCandidates will develop a Retrieval -Augmented Generation (RAG) chatbot capable of \n\u201cagentic\u201d behavior . The chatbot should leverage Crew AI (or similar agent framework) to \nhandle agent ic tasks and incorporate Chonkie  for chunking source documents. The \nsolution must also include an evaluation step using DeepEval  or a comparable framework \nto measure retrieval and generation performance.  \nDeadline:  \nYou have 66 hours from receiving this message to submit a link to a GitHub repository \ncontaining your full solution. Any code commits beyond this time will not be considered. A \nnon-compiling solution does not necessarily disqualify you \u2014clear explanations of your \ndesign and problem -solving approach are highly valuable.  \nGetting Started:  \nFeel free to explore existing tutorials, guides, or code snippets on agentic RAG, Crew AI, \nChonkie, and DeepEval. Your solution will be assessed on design clarity, rationale, and \ndocumentation as much as on functionality.  In particular, detailing why and how you \nemploy Chonkie, DeepEval, and other tools is highly recommended.  \n \n \n \n \n\n\n \nEvaluation Criteria  \n1. End-to-End RAG Chatbot  \n\u2022 Agentic Behavior:  Demonstrate how your choice of agent fram ework is integrated \nto manage dynamic tasks (e.g., deciding whether to retrieve more context, refine the \nprompt, or request clarifications).  \n\u2022 Chunking Implementation:  Use Chonkie to partition documents into optimally \nsized chunks. Clearly document your chunking strategy and explain its impact on \nretrieval (use some advance semantic ch unking for bonus points).  \n\u2022 Architecture:  Choose a vector database (e.g., Qdrant, Milvus, Pinecone), agent \nframework, and evaluation tool that best suit your project. Provide reasoning behind \neach choice and how you integrated them, highlighting factors such as scalability, \nease of setup, or uniqu e features.  \n2. RAG Evaluation with DeepEval (or Similar)  \nYour assessment should show both retrieval and generation quality:  \n1. Context Retrieval  \no Contextual Precision:  How often the system retrieves highly relevant \nchunks.  \no Contextual Recall:  Ensures that the system captures all necessary context \nfrom the database.  \no Contextual Relevancy:  Measures how closely the returned context aligns \nwith the user query.  \n2. Content Generation  \no Answer Relevancy:  Checks if the chatbot\u2019s generated response truly \naddresses user inquiries.  \no Faithfulness:  Confirms the response remains grounded in the retrieved \ncontext, avoiding extraneous or false details.  \nYou are welcome to add extra metrics or methods for evaluation if they offer additional \ninsights (e.g., side -by-side comparisons, user testing).  \nHAPPY CODING!!!                          \n\n",
        "metadata": {
          "source": "/home/goutham/Aparavi/data/sample_docs/AI+QA_New_Hire_Challenge.pdf",
          "file_name": "AI+QA_New_Hire_Challenge.pdf",
          "chunk_id": 0,
          "chunk_type": "token",
          "token_count": 765
        },
        "score": 0.16673146
      },
      {
        "text": " \nNew Hire Challenge  \n \nQuality Assurance Engineer \u2013 AI Product Line  \nAgentic RAG Chatbot Challenge with Crew AI, Chonkie, and DeepEval  \n \nObjective:  \nCandidates will develop a Retrieval -Augmented Generation (RAG) chatbot capable of \n\u201cagentic\u201d behavior . The chatbot should leverage Crew AI (or similar agent framework) to \nhandle agent ic tasks and incorporate Chonkie  for chunking source documents. The \nsolution must also include an evaluation step using DeepEval  or a comparable framework \nto measure retrieval and generation performance.  \nDeadline:  \nYou have 66 hours from receiving this message to submit a link to a GitHub repository \ncontaining your full solution. Any code commits beyond this time will not be considered. A \nnon-compiling solution does not necessarily disqualify you \u2014clear explanations of your \ndesign and problem -solving approach are highly valuable.  \nGetting Started:  \nFeel free to explore existing tutorials, guides, or code snippets on agentic RAG, Crew AI, \nChonkie, and DeepEval. Your solution will be assessed on design clarity, rationale, and \ndocumentation as much as on functionality.  In particular, detailing why and how you \nemploy Chonkie, DeepEval, and other tools is highly recommended.  \n \n \n \n \n\n\n \nEvaluation Criteria  \n1. End-to-End RAG Chatbot  \n\u2022 Agentic Behavior:  Demonstrate how your choice of agent fram ework is integrated \nto manage dynamic tasks (e.g., deciding whether to retrieve more context, refine the \nprompt, or request clarifications).  \n\u2022 Chunking Implementation:  Use Chonkie to partition documents into optimally \nsized chunks. Clearly document your chunking strategy and explain its impact on \nretrieval (use some advance semantic ch unking for bonus points).  \n\u2022 Architecture:  Choose a vector database (e.g., Qdrant, Milvus, Pinecone), agent \nframework, and evaluation tool that best suit your project. Provide reasoning behind \neach choice and how you integrated them, highlighting factors such as scalability, \nease of setup, or uniqu e features.  \n2. RAG Evaluation with DeepEval (or Similar)  \nYour assessment should show both retrieval and generation quality:  \n1. Context Retrieval  \no Contextual Precision:  How often the system retrieves highly relevant \nchunks.  \no Contextual Recall:  Ensures that the system captures all necessary context \nfrom the database.  \no Contextual Relevancy:  Measures how closely the returned context aligns \nwith the user query.  \n2. Content Generation  \no Answer Relevancy:  Checks if the chatbot\u2019s generated response truly \naddresses user inquiries.  \no Faithfulness:  Confirms the response remains grounded in the retrieved \ncontext, avoiding extraneous or false details.  \nYou are welcome to add extra metrics or methods for evaluation if they offer additional \ninsights (e.g., side -by-side comparisons, user testing).  \nHAPPY CODING!!!                          \n\n",
        "metadata": {
          "source": "/home/goutham/Aparavi/data/sample_docs/AI+QA_New_Hire_Challenge.pdf",
          "file_name": "AI+QA_New_Hire_Challenge.pdf",
          "chunk_id": 0,
          "chunk_type": "token",
          "token_count": 765
        },
        "score": 0.16673146
      },
      {
        "text": " \nNew Hire Challenge  \n \nQuality Assurance Engineer \u2013 AI Product Line  \nAgentic RAG Chatbot Challenge with Crew AI, Chonkie, and DeepEval  \n \nObjective:  \nCandidates will develop a Retrieval -Augmented Generation (RAG) chatbot capable of \n\u201cagentic\u201d behavior . The chatbot should leverage Crew AI (or similar agent framework) to \nhandle agent ic tasks and incorporate Chonkie  for chunking source documents. The \nsolution must also include an evaluation step using DeepEval  or a comparable framework \nto measure retrieval and generation performance.  \nDeadline:  \nYou have 66 hours from receiving this message to submit a link to a GitHub repository \ncontaining your full solution. Any code commits beyond this time will not be considered. A \nnon-compiling solution does not necessarily disqualify you \u2014clear explanations of your \ndesign and problem -solving approach are highly valuable.  \nGetting Started:  \nFeel free to explore existing tutorials, guides, or code snippets on agentic RAG, Crew AI, \nChonkie, and DeepEval. Your solution will be assessed on design clarity, rationale, and \ndocumentation as much as on functionality.  In particular, detailing why and how you \nemploy Chonkie, DeepEval, and other tools is highly recommended.  \n \n \n \n \n\n\n \nEvaluation Criteria  \n1. End-to-End RAG Chatbot  \n\u2022 Agentic Behavior:  Demonstrate how your choice of agent fram ework is integrated \nto manage dynamic tasks (e.g., deciding whether to retrieve more context, refine the \nprompt, or request clarifications).  \n\u2022 Chunking Implementation:  Use Chonkie to partition documents into optimally \nsized chunks. Clearly document your chunking strategy and explain its impact on \nretrieval (use some advance semantic ch unking for bonus points).  \n\u2022 Architecture:  Choose a vector database (e.g., Qdrant, Milvus, Pinecone), agent \nframework, and evaluation tool that best suit your project. Provide reasoning behind \neach choice and how you integrated them, highlighting factors such as scalability, \nease of setup, or uniqu e features.  \n2. RAG Evaluation with DeepEval (or Similar)  \nYour assessment should show both retrieval and generation quality:  \n1. Context Retrieval  \no Contextual Precision:  How often the system retrieves highly relevant \nchunks.  \no Contextual Recall:  Ensures that the system captures all necessary context \nfrom the database.  \no Contextual Relevancy:  Measures how closely the returned context aligns \nwith the user query.  \n2. Content Generation  \no Answer Relevancy:  Checks if the chatbot\u2019s generated response truly \naddresses user inquiries.  \no Faithfulness:  Confirms the response remains grounded in the retrieved \ncontext, avoiding extraneous or false details.  \nYou are welcome to add extra metrics or methods for evaluation if they offer additional \ninsights (e.g., side -by-side comparisons, user testing).  \nHAPPY CODING!!!                          \n\n",
        "metadata": {
          "source": "/tmp/tmp3t_2s3gu.pdf",
          "file_name": "tmp3t_2s3gu.pdf",
          "chunk_id": 0,
          "chunk_type": "token",
          "token_count": 765
        },
        "score": 0.16673146
      },
      {
        "text": " \nNew Hire Challenge  \n \nQuality Assurance Engineer \u2013 AI Product Line  \nAgentic RAG Chatbot Challenge with Crew AI, Chonkie, and DeepEval  \n \nObjective:  \nCandidates will develop a Retrieval -Augmented Generation (RAG) chatbot capable of \n\u201cagentic\u201d behavior . The chatbot should leverage Crew AI (or similar agent framework) to \nhandle agent ic tasks and incorporate Chonkie  for chunking source documents. The \nsolution must also include an evaluation step using DeepEval  or a comparable framework \nto measure retrieval and generation performance.  \nDeadline:  \nYou have 66 hours from receiving this message to submit a link to a GitHub repository \ncontaining your full solution. Any code commits beyond this time will not be considered. A \nnon-compiling solution does not necessarily disqualify you \u2014clear explanations of your \ndesign and problem -solving approach are highly valuable.  \nGetting Started:  \nFeel free to explore existing tutorials, guides, or code snippets on agentic RAG, Crew AI, \nChonkie, and DeepEval. Your solution will be assessed on design clarity, rationale, and \ndocumentation as much as on functionality.  In particular, detailing why and how you \nemploy Chonkie, DeepEval, and other tools is highly recommended.  \n \n \n \n \n\n\n \nEvaluation Criteria  \n1. End-to-End RAG Chatbot  \n\u2022 Agentic Behavior:  Demonstrate how your choice of agent fram ework is integrated \nto manage dynamic tasks (e.g., deciding whether to retrieve more context, refine the \nprompt, or request clarifications).  \n\u2022 Chunking Implementation:  Use Chonkie to partition documents into optimally \nsized chunks. Clearly document your chunking strategy and explain its impact on \nretrieval (use some advance semantic ch unking for bonus points).  \n\u2022 Architecture:  Choose a vector database (e.g., Qdrant, Milvus, Pinecone), agent \nframework, and evaluation tool that best suit your project. Provide reasoning behind \neach choice and how you integrated them, highlighting factors such as scalability, \nease of setup, or uniqu e features.  \n2. RAG Evaluation with DeepEval (or Similar)  \nYour assessment should show both retrieval and generation quality:  \n1. Context Retrieval  \no Contextual Precision:  How often the system retrieves highly relevant \nchunks.  \no Contextual Recall:  Ensures that the system captures all necessary context \nfrom the database.  \no Contextual Relevancy:  Measures how closely the returned context aligns \nwith the user query.  \n2. Content Generation  \no Answer Relevancy:  Checks if the chatbot\u2019s generated response truly \naddresses user inquiries.  \no Faithfulness:  Confirms the response remains grounded in the retrieved \ncontext, avoiding extraneous or false details.  \nYou are welcome to add extra metrics or methods for evaluation if they offer additional \ninsights (e.g., side -by-side comparisons, user testing).  \nHAPPY CODING!!!                          \n\n",
        "metadata": {
          "source": "/tmp/tmpxvcwsr41.pdf",
          "file_name": "tmpxvcwsr41.pdf",
          "chunk_id": 0,
          "chunk_type": "token",
          "token_count": 765
        },
        "score": 0.16673146
      },
      {
        "text": " \nNew Hire Challenge  \n \nQuality Assurance Engineer \u2013 AI Product Line  \nAgentic RAG Chatbot Challenge with Crew AI, Chonkie, and DeepEval  \n \nObjective:  \nCandidates will develop a Retrieval -Augmented Generation (RAG) chatbot capable of \n\u201cagentic\u201d behavior . The chatbot should leverage Crew AI (or similar agent framework) to \nhandle agent ic tasks and incorporate Chonkie  for chunking source documents. The \nsolution must also include an evaluation step using DeepEval  or a comparable framework \nto measure retrieval and generation performance.  \nDeadline:  \nYou have 66 hours from receiving this message to submit a link to a GitHub repository \ncontaining your full solution. Any code commits beyond this time will not be considered. A \nnon-compiling solution does not necessarily disqualify you \u2014clear explanations of your \ndesign and problem -solving approach are highly valuable.  \nGetting Started:  \nFeel free to explore existing tutorials, guides, or code snippets on agentic RAG, Crew AI, \nChonkie, and DeepEval. Your solution will be assessed on design clarity, rationale, and \ndocumentation as much as on functionality.  In particular, detailing why and how you \nemploy Chonkie, DeepEval, and other tools is highly recommended.  \n \n \n \n \n\n\n \nEvaluation Criteria  \n1. End-to-End RAG Chatbot  \n\u2022 Agentic Behavior:  Demonstrate how your choice of agent fram ework is integrated \nto manage dynamic tasks (e.g., deciding whether to retrieve more context, refine the \nprompt, or request clarifications).  \n\u2022 Chunking Implementation:  Use Chonkie to partition documents into optimally \nsized chunks. Clearly document your chunking strategy and explain its impact on \nretrieval (use some advance semantic ch unking for bonus points).  \n\u2022 Architecture:  Choose a vector database (e.g., Qdrant, Milvus, Pinecone), agent \nframework, and evaluation tool that best suit your project. Provide reasoning behind \neach choice and how you integrated them, highlighting factors such as scalability, \nease of setup, or uniqu e features.  \n2. RAG Evaluation with DeepEval (or Similar)  \nYour assessment should show both retrieval and generation quality:  \n1. Context Retrieval  \no Contextual Precision:  How often the system retrieves highly relevant \nchunks.  \no Contextual Recall:  Ensures that the system captures all necessary context \nfrom the database.  \no Contextual Relevancy:  Measures how closely the returned context aligns \nwith the user query.  \n2. Content Generation  \no Answer Relevancy:  Checks if the chatbot\u2019s generated response truly \naddresses user inquiries.  \no Faithfulness:  Confirms the response remains grounded in the retrieved \ncontext, avoiding extraneous or false details.  \nYou are welcome to add extra metrics or methods for evaluation if they offer additional \ninsights (e.g., side -by-side comparisons, user testing).  \nHAPPY CODING!!!                          \n\n",
        "metadata": {
          "source": "/home/goutham/Aparavi/data/sample_docs/AI+QA_New_Hire_Challenge.pdf",
          "file_name": "AI+QA_New_Hire_Challenge.pdf",
          "chunk_id": 0,
          "chunk_type": "token",
          "token_count": 765
        },
        "score": 0.16673146
      }
    ],
    "analysis": "1. Core Information Need:\n   - The user is seeking guidance on actions or instructions derived from a specific PDF document.\n\n2. Key Concepts that Should be Present in Relevant Documents:\n   - Instructions or tasks explicitly stated in the PDF.\n   - Context or subject matter of the PDF (e.g., educational content, procedural guidelines, technical information).\n   - Any specific sections or pages within the PDF that outline needed actions.\n   - Clarification or summary of content within the PDF that details the action steps to be taken.\n\n3. Constraints or Preferences Mentioned:\n   - The user implicitly requires information that is pertinent to the content of the PDF, suggesting a need for specific, actionable insights rather than general advice. \n\nThis structured analysis highlights the user's need for precise instructions from the PDF, ensuring that any relevant documents retrieved will align closely with those requirements."
  },
  "answer": "According to the provided context, you need to develop a Retrieval-Augmented Generation (RAG) chatbot capable of \"agentic\" behavior. This chatbot should leverage Crew AI (or a similar agent framework) to handle agentic tasks and incorporate Chonkie for chunking source documents. Additionally, you are required to include an evaluation step using DeepEval or a comparable framework to measure the retrieval and generation performance of your solution. The deadline for submission of your solution is 66 hours from the time you receive the challenge. You are expected to submit a link to a GitHub repository containing your full solution within the given timeframe. Remember, a non-compiling solution does not disqualify you, but clear explanations of your design and problem-solving approach are highly valued. Your solution will be assessed not only based on functionality but also on design clarity, rationale, and documentation. It is recommended to provide detailed explanations on why and how you employ Chonkie, DeepEval, and other tools within your solution. The evaluation criteria include demonstrating agentic behavior, employing Chonkie for chunking, selecting appropriate architecture components, and evaluating the retrieval and generation quality using metrics such as contextual precision, recall, relevancy, answer relevancy, and faithfulness. Additionally, you are welcome to include extra metrics or methods for evaluation if they offer additional insights.",
  "response": "According to the provided context, you need to develop a Retrieval-Augmented Generation (RAG) chatbot capable of \"agentic\" behavior. This chatbot should leverage Crew AI (or a similar agent framework) to handle agentic tasks and incorporate Chonkie for chunking source documents. Additionally, you are required to include an evaluation step using DeepEval or a comparable framework to measure the retrieval and generation performance of your solution. The deadline for submission of your solution is 66 hours from the time you receive the challenge. You are expected to submit a link to a GitHub repository containing your full solution within the given timeframe. Remember, a non-compiling solution does not disqualify you, but clear explanations of your design and problem-solving approach are highly valued. Your solution will be assessed not only based on functionality but also on design clarity, rationale, and documentation. It is recommended to provide detailed explanations on why and how you employ Chonkie, DeepEval, and other tools within your solution. The evaluation criteria include demonstrating agentic behavior, employing Chonkie for chunking, selecting appropriate architecture components, and evaluating the retrieval and generation quality using metrics such as contextual precision, recall, relevancy, answer relevancy, and faithfulness. Additionally, you are welcome to include extra metrics or methods for evaluation if they offer additional insights.",
  "evaluation": {
    "query": "what do i need to do according to the pdf?",
    "answer": "According to the provided context, you need to develop a Retrieval-Augmented Generation (RAG) chatbot capable of \"agentic\" behavior. This chatbot should leverage Crew AI (or a similar agent framework) to handle agentic tasks and incorporate Chonkie for chunking source documents. Additionally, you are required to include an evaluation step using DeepEval or a comparable framework to measure the retrieval and generation performance of your solution. The deadline for submission of your solution is 66 hours from the time you receive the challenge. You are expected to submit a link to a GitHub repository containing your full solution within the given timeframe. Remember, a non-compiling solution does not disqualify you, but clear explanations of your design and problem-solving approach are highly valued. Your solution will be assessed not only based on functionality but also on design clarity, rationale, and documentation. It is recommended to provide detailed explanations on why and how you employ Chonkie, DeepEval, and other tools within your solution. The evaluation criteria include demonstrating agentic behavior, employing Chonkie for chunking, selecting appropriate architecture components, and evaluating the retrieval and generation quality using metrics such as contextual precision, recall, relevancy, answer relevancy, and faithfulness. Additionally, you are welcome to include extra metrics or methods for evaluation if they offer additional insights.",
    "metrics": {
      "answer_relevancy": 1.0,
      "contextual_precision": 0,
      "contextual_recall": 1.0,
      "contextual_relevancy": 0.2,
      "faithfulness": 1.0
    }
  },
  "timestamp": 1741057117.9922154,
  "processing_time": 12.436795711517334
}